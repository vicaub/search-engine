{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Search-Engine - Fondement de la Recherche d'Information-WEB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements installation\n",
    "Run:\n",
    "```\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche 1 : Création d’un index inversé et moteur de recherche booléen et vectorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Traitements linguistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation des collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CACM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importation de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cacm():\n",
    "    file_name = \"./Data/CACM/cacm.all\"\n",
    "    file = open(file_name)\n",
    "    documents = []\n",
    "    for line in file.readlines():\n",
    "        curent_section = -1\n",
    "        if line[0] == '.':\n",
    "            if line[1] == 'I':\n",
    "                documents.append([\"\", \"\", \"\"])\n",
    "            elif line[1] == 'T':\n",
    "                current_section = 0\n",
    "            elif line[1] == 'W':\n",
    "                current_section = 1\n",
    "            elif line[1] == 'K':\n",
    "                current_section = 2\n",
    "            else:\n",
    "                current_section = -1\n",
    "        else:\n",
    "            if current_section >= 0:\n",
    "                if len(documents[-1][current_section]) == 0:\n",
    "                    documents[-1][current_section] += line.strip('\\n')\n",
    "                else:\n",
    "                    documents[-1][current_section] += \" \" + line.strip('\\n')\n",
    "    file.close()  \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_forbidden_words():\n",
    "    file_name = \"./Data/CACM/common_words\"\n",
    "    file = open(file_name)\n",
    "    words = []\n",
    "    for word in file.readlines():\n",
    "        words.append(word.strip().lower())\n",
    "    file.close()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def tokenize_CACM(documents):\n",
    "    d = {}\n",
    "    forbidden_words = read_forbidden_words()\n",
    "    for i in range(len(documents)):\n",
    "        document = documents[i]\n",
    "        for text in document:\n",
    "            if len(text) > 0:\n",
    "                tokens = nltk.word_tokenize(text)\n",
    "                for token in tokens:\n",
    "                    if token.lower() not in forbidden_words and len(token) > 0:\n",
    "                        if token.lower() in d:\n",
    "                            d[token.lower()].append(i+1)\n",
    "                        else:\n",
    "                            d[token.lower()] = [i+1]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize without NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize_no_nltk_CACM(documents):\n",
    "    d = {}\n",
    "    forbidden_words = read_forbidden_words()\n",
    "    for i in range(len(documents)):\n",
    "        document = documents[i]\n",
    "        for text in document:\n",
    "            if len(text) > 0:\n",
    "                tokens = re.compile(\"[^0-9a-zA-Z]\").split(text)\n",
    "                for token in tokens:\n",
    "                    if token.lower() not in forbidden_words and len(token) > 0:\n",
    "                        if token.lower() in d:\n",
    "                            d[token.lower()].append(i+1)\n",
    "                        else:\n",
    "                            d[token.lower()] = [i+1]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACM_documents = read_cacm()\n",
    "CACM_tokens_NLTK = tokenize_CACM(CACM_documents)\n",
    "CACM_tokens = tokenize_no_nltk_CACM(CACM_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS276\n",
    "use CACM stoplist and Landcaster stemmer\n",
    "https://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_cs276():\n",
    "    \"\"\"\n",
    "    returns list for document tokens\n",
    "    \"\"\"\n",
    "    collection_path = \"./Data/CS276/pa1-data/\"\n",
    "    documents = []\n",
    "    \n",
    "    # getting all sub folders which are not hidden\n",
    "    sub_dirs = [collection_path + path + \"/\" for path in os.listdir(collection_path) if path[0] != \".\"]\n",
    "    \n",
    "    for sub_path in sub_dirs:\n",
    "        for article in os.listdir(sub_path):\n",
    "            if article[0] != \".\": # check if it's not a hidden file\n",
    "                with open(sub_path + article) as f:\n",
    "                    documents.append(f.read().strip().split())\n",
    "    \n",
    "    return documents\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def tokenize_CS276(documents):\n",
    "    \"\"\"\n",
    "    returns a dictionnary -> key: token name, value: list of document ids where token appear\n",
    "    \"\"\"\n",
    "    tokens = {}\n",
    "    st = LancasterStemmer()\n",
    "    forbidden_words = read_forbidden_words()\n",
    "    for i in range(len(documents)):\n",
    "        for token in documents[i]:\n",
    "            if token.lower() not in forbidden_words and len(token.lower()) > 0:\n",
    "                if token.lower() in tokens.keys():\n",
    "                    tokens[stem_token].append(i)\n",
    "                else:\n",
    "                    # stemisation\n",
    "                    stem_token = st.stem(token.lower())\n",
    "                    if stem_token not in tokens.keys():\n",
    "                        tokens[stem_token] = [i]\n",
    "                    else:\n",
    "                        tokens[stem_token].append(i)\n",
    "        if i%1000 == 0:\n",
    "            print(\"Processing document {}/{}          \".format(str(i), str(len(documents))), end=\"\\r\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(\"CS276_docs.pickle\"):\n",
    "    CS276_docs = pickle.load(open(\"CS276_docs.pickle\", 'rb'))\n",
    "else:\n",
    "    CS276_docs = read_cs276()\n",
    "    pickle.dump(CS276_docs, open(\"CS276_docs.pickle\", 'wb'))\n",
    "\n",
    "if os.path.exists(\"CS276_tokens.pickle\"):\n",
    "    CS276_tokens = pickle.load(open(\"CS276_tokens.pickle\", 'rb'))\n",
    "else:\n",
    "    CS276_tokens = tokenize_CS276(CS276_docs)\n",
    "    pickle.dump(CS276_tokens, open(\"CS276_tokens.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de tokens pour CACM (sans NLTK):\")\n",
    "T1 = sum([len(L) for L in CACM_tokens])\n",
    "print(T1)\n",
    "\n",
    "print(\"Nombre de tokens pour CACM (NLTK):\")\n",
    "T1 = sum([len(L) for L in CACM_tokens_NLTK])\n",
    "print(T1)\n",
    "\n",
    "print(\"Nombre de tokens pour CS276:\")\n",
    "T1 = sum([len(L) for L in CS276_tokens])\n",
    "print(T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulaire pour CACM: \")\n",
    "M1 = len(CACM_tokens)\n",
    "print(M1)\n",
    "# print(CACM_tokens.keys())\n",
    "\n",
    "print(\"Vocabulaire pour CACM: \")\n",
    "M1 = len(CACM_tokens_NLTK)\n",
    "print(M1)\n",
    "# print(CACM_tokens.keys())\n",
    "\n",
    "print(\"Vocabulaire pour CS276_tokens: \")\n",
    "M1 = len(CS276_tokens)\n",
    "print(M1)\n",
    "# print(CS276_tokens.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loi Heap et estimation pour 1M vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_heap_law(tokens_full, tokens_half, collection_name):\n",
    "    print(\"heap low for collection\", collection_name)\n",
    "    T1 = sum([len(L) for L in tokens_full])\n",
    "    T2 = sum([len(L) for L in tokens_half])\n",
    "\n",
    "    M1 = len(tokens_full)\n",
    "    M2 = len(tokens_half)\n",
    "    \n",
    "    from math import log, pow\n",
    "    b = log(M1/M2)/log(T1/T2)\n",
    "    k = M1/(pow(T1, b))\n",
    "    print(\"K = {}, b = {}\".format(k, b))\n",
    "    \n",
    "    print('Pour 1 million de tokens, vocabulaire :')\n",
    "    print(int(k * pow(1e6, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_half_cacm():\n",
    "    documents = read_cacm()\n",
    "    return documents[:len(documents)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_half = tokenize_no_nltk_CACM(CACM_documents[:len(CACM_documents)//2])\n",
    "print_heap_law(CACM_tokens, d_half, \"CACM without NLTK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_half = tokenize_CACM(read_half_cacm())\n",
    "print_heap_law(CACM_tokens_NLTK, d_half, \"CACM with NLTK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS276_tokens_heap = CS276_tokens.copy()\n",
    "ids_to_delete = list(range(len(CS276_docs)//2))\n",
    "\n",
    "for document_ids in CS276_tokens_heap.values():\n",
    "    filter(lambda d_id: d_id not in ids_to_delete, document_ids)\n",
    "\n",
    "print_heap_law(CS276_tokens, d_half, \"CS276\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphe fréquence / rang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def word_frequency(tokens):\n",
    "    freq = {}\n",
    "    words = tokens.keys()\n",
    "    for word in words:\n",
    "        freq[word] = len(tokens[word])\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_CACM = word_frequency(CACM_tokens)\n",
    "freq_CACM_NLTK = word_frequency(CACM_tokens_NLTK)\n",
    "freq_CS276 = word_frequency(CS276_tokens)\n",
    "\n",
    "# print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_sorter():\n",
    "    def __init__(self, word, frequency):\n",
    "        self.word = word\n",
    "        self.f = frequency\n",
    "    def __lt__(self, other):\n",
    "        return self.f > other.f\n",
    "    def __repr__(self):\n",
    "        return str(self.word) + \" : \" + str(self.f)\n",
    "list_word = [Token_sorter(word, freq[word]) for word in freq.keys()]\n",
    "list_word.sort()\n",
    "print(list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.subplot(311)\n",
    "list_word = [Token_sorter(word, freq_CACM[word]) for word in freq_CACM.keys()]\n",
    "list_word.sort()\n",
    "X = range(1, len(list_word) + 1)\n",
    "Y = [word.f for word in list_word]\n",
    "plt.plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, array\n",
    "X2= log(array(X))\n",
    "Y2 = log(array(Y))\n",
    "plt.plot(X2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplot(312)\n",
    "list_word = [Token_sorter(word, freq_CACM_NLTK[word]) for word in freq_CACM_NLTK.keys()]\n",
    "list_word.sort()\n",
    "X = range(1, len(list_word) + 1)\n",
    "Y = [word.f for word in list_word]\n",
    "plt.plot(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, array\n",
    "X2= log(array(X))\n",
    "Y2 = log(array(Y))\n",
    "plt.plot(X2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplot(313)\n",
    "list_word = [Token_sorter(word, freq_CS276[word]) for word in freq_CS276.keys()]\n",
    "list_word.sort()\n",
    "X = range(1, len(list_word) + 1)\n",
    "Y = [word.f for word in list_word]\n",
    "plt.plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, array\n",
    "X2= log(array(X))\n",
    "Y2 = log(array(Y))\n",
    "plt.plot(X2, Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INDEX INVERSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term = {}\n",
    "i = 0\n",
    "for token in tokenize_no_nltk_CACM(CACM_documents):\n",
    "    if token not in dict_term:\n",
    "        dict_term[token] = i\n",
    "        i += 1\n",
    "\n",
    "def write_dict(file_name, liste):\n",
    "    f = open(file_name, \"w\")\n",
    "    for termId, list_docID in liste:\n",
    "        f.write(str(termId))\n",
    "        f.write(\" \")\n",
    "        for docID in list_docID:\n",
    "            f.write(str(docID) + \" \")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def read_line(line):\n",
    "    nombres = line.split(' ')\n",
    "    nombres.pop() #on enlève le \\n\n",
    "    nombres = list(map(int, nombres))\n",
    "    termID = nombres[0]\n",
    "    list_docID = nombres[1:]\n",
    "    return (termID, list_docID)\n",
    "\n",
    "def add_token_to_term_list(term_list, token, new_list_docID):\n",
    "    new_termID = dict_term[token]\n",
    "    for termID, list_docID in term_list:\n",
    "        if termID == new_termID:\n",
    "            for docID in new_list_docID:\n",
    "                if docID not in list_docID:\n",
    "                    list_docID.append(docID)\n",
    "            return\n",
    "    term_list.append((new_termID, list(set(new_list_docID)))) # On enlève les doublons\n",
    "    \n",
    "\n",
    "def invert_block(documents, indice_premier, taille):\n",
    "    tokens = tokenize_no_nltk_CACM(documents[indice_premier:indice_premier + taille])\n",
    "    index_inverse_block = []\n",
    "    for token, list_docID in tokens.items():\n",
    "        add_token_to_term_list(index_inverse_block, token, [docID + indice_premier for docID in list_docID])\n",
    "    return index_inverse_block\n",
    "\n",
    "def trouver_terme_mini(liste_lignes):\n",
    "    minimum = float('inf')\n",
    "    min_indices = []\n",
    "    for i in range(len(liste_lignes)):\n",
    "        if liste_lignes[i] != \"\":\n",
    "            termID = read_line(liste_lignes[i])[0]\n",
    "            if termID < minimum:\n",
    "                minimum = termID\n",
    "                min_indices = [i]\n",
    "            elif termID == minimum:\n",
    "                min_indices.append(i)\n",
    "    return min_indices\n",
    "    \n",
    "\n",
    "def index_inverse_global(documents, taille, file_name):\n",
    "    indice = 0\n",
    "    n = 0\n",
    "    while indice < len(documents):\n",
    "        index_inverse_block = invert_block(documents, indice, taille)\n",
    "        index_inverse_block.sort(key = lambda x: x[0])\n",
    "        \n",
    "        write_dict(file_name + \"_\" + str(n), index_inverse_block)\n",
    "        n += 1\n",
    "        indice += taille\n",
    "    files = [open(file_name + \"_\" + str(i)) for i in range(n)]\n",
    "    lignes_courantes = [files[i].readline() for i in range(n)]\n",
    "    blocks_finis = 0\n",
    "    final_dict = open(file_name, \"w\")\n",
    "    while blocks_finis < n:\n",
    "        min_indices = trouver_terme_mini(lignes_courantes)\n",
    "        termID = read_line(lignes_courantes[min_indices[0]])[0]\n",
    "        docIDs = []\n",
    "        for i in min_indices:\n",
    "            docIDs += read_line(lignes_courantes[i])[1]\n",
    "        final_dict.write(str(termID))\n",
    "        final_dict.write(\" \")\n",
    "        for docID in docIDs:\n",
    "            final_dict.write(str(docID) + \" \")\n",
    "        final_dict.write(\"\\n\")\n",
    "        for i in min_indices:\n",
    "            #print(lignes_courantes)\n",
    "            #print(min_indices)\n",
    "            lignes_courantes[i] = files[i].readline()\n",
    "            #print(lignes_courantes)\n",
    "            #print()\n",
    "            if lignes_courantes[i] == '':\n",
    "                blocks_finis +=1\n",
    "    [files[i].close() for i in range(n)]\n",
    "    final_dict.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inverse_global(CACM_documents, 1000, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_with_frequency(file_name, liste):\n",
    "    f = open(file_name, \"w\")\n",
    "    for termId, list_docID in liste:\n",
    "        f.write(str(termId))\n",
    "        f.write(\" \")\n",
    "        for docID in list_docID:\n",
    "            f.write(str(docID[0]) + \",\" + str(docID[1]) + \" \")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def read_line_with_frequency(line):\n",
    "    nombres = line.split(' ')\n",
    "    nombres.pop() #on enlève le \\n\n",
    "    termID = int(nombres[0])\n",
    "    list_docID = []\n",
    "    for couple in nombres[1:]:\n",
    "        docID, freq = map(int, couple.split(\",\"))\n",
    "        list_docID.append([docID, freq])\n",
    "    return (termID, list_docID)\n",
    "\n",
    "def add_token_to_term_list_with_frequency(term_list, token, new_list_docID):\n",
    "    new_termID = dict_term[token]\n",
    "    for termID, list_docID in term_list:\n",
    "        if termID == new_termID:\n",
    "            for docID in new_list_docID:\n",
    "                deja_present = False\n",
    "                for old_docID, freq in list_docID:\n",
    "                    if docID == old_docID:\n",
    "                        freq += 1\n",
    "                        deja_present = True\n",
    "                        break\n",
    "                if not deja_present:\n",
    "                    list_docID.append([doc_ID, 1])\n",
    "            return\n",
    "    docID_with_freq = []\n",
    "    for docID in new_list_docID:\n",
    "        if docID not in [x[0] for x in docID_with_freq]:\n",
    "            docID_with_freq.append([docID, new_list_docID.count(docID)])\n",
    "    term_list.append((new_termID, docID_with_freq)) # On enlève les doublons\n",
    "    \n",
    "\n",
    "def invert_block_with_frequency(documents, indice_premier, taille):\n",
    "    tokens = tokenize_no_nltk_CACM(documents[indice_premier:indice_premier + taille])\n",
    "    index_inverse_block = []\n",
    "    for token, list_docID in tokens.items():\n",
    "        add_token_to_term_list_with_frequency(index_inverse_block, token, [docID + indice_premier for docID in list_docID])\n",
    "    return index_inverse_block\n",
    "\n",
    "def trouver_terme_mini_with_frequency(liste_lignes):\n",
    "    minimum = float('inf')\n",
    "    min_indices = []\n",
    "    for i in range(len(liste_lignes)):\n",
    "        if liste_lignes[i] != \"\":\n",
    "            termID = read_line_with_frequency(liste_lignes[i])[0]\n",
    "            if termID < minimum:\n",
    "                minimum = termID\n",
    "                min_indices = [i]\n",
    "            elif termID == minimum:\n",
    "                min_indices.append(i)\n",
    "    return min_indices\n",
    "    \n",
    "\n",
    "def index_inverse_global_with_frequency(documents, taille, file_name):\n",
    "    indice = 0\n",
    "    n = 0\n",
    "    while indice < len(documents):\n",
    "        index_inverse_block = invert_block_with_frequency(documents, indice, taille)\n",
    "        index_inverse_block.sort(key = lambda x: x[0])\n",
    "        \n",
    "        write_dict_with_frequency(file_name + \"_\" + str(n), index_inverse_block)\n",
    "        n += 1\n",
    "        indice += taille\n",
    "    files = [open(file_name + \"_\" + str(i)) for i in range(n)]\n",
    "    lignes_courantes = [files[i].readline() for i in range(n)]\n",
    "    blocks_finis = 0\n",
    "    final_dict = open(file_name, \"w\")\n",
    "    while blocks_finis < n:\n",
    "        min_indices = trouver_terme_mini_with_frequency(lignes_courantes)\n",
    "        termID = read_line_with_frequency(lignes_courantes[min_indices[0]])[0]\n",
    "        docIDs = []\n",
    "        for i in min_indices:\n",
    "            docIDs += read_line_with_frequency(lignes_courantes[i])[1]\n",
    "        final_dict.write(str(termID))\n",
    "        final_dict.write(\" \")\n",
    "        for docID in docIDs:\n",
    "            final_dict.write(str(docID[0]) + \",\" + str(docID[1]) + \" \")\n",
    "        final_dict.write(\"\\n\")\n",
    "        for i in min_indices:\n",
    "            lignes_courantes[i] = files[i].readline()\n",
    "            if lignes_courantes[i] == '':\n",
    "                blocks_finis +=1\n",
    "    [files[i].close() for i in range(n)]\n",
    "    final_dict.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inverse_global_with_frequency(CACM_documents, 1000, \"output frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
